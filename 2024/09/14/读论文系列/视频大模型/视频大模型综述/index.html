<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><title>Video Understanding with LLMs | Orion Blog</title><meta name="author" content="Orion"><meta name="description" content="Here's Orion"><meta name="keywords" content="技术博客,Orion,BUPT北京邮电大学,本科生,人工智能,AI算法工程师,机器学习,深度学习,计算机视觉,自然语言处理,NLP,CV,大模型"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><link rel="icon" href="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409190134514.png"><link rel="preload" as="image" href="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409181543316.png"><link rel="preconnect" href="https://s4.zstatic.net"><script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" media="only x" onload='this.onload=null,this.media="all"'><link rel="preconnect" href="https://fonts.googleapis.cn"><link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" media="only x" onload='this.onload=null,this.media="all"'><script>const mixins={}</script><script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script><script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"><script src="/js/lib/highlight.js"></script><script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script><script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css"><script src="/js/lib/math.js"></script><script src="/js/lib/preview.js"></script><script src="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline.min.css"><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline-meta.min.css"><script src="https://cdn.staticfile.org/animejs/3.2.1/anime.min.js"></script><link rel="stylesheet" href="/css/main.css"><link rel="preconnect" href="https://static-argvchs.netlify.app"><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:yellow;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style></head><body><div id="layout"><transition name="fade"><div id="loading" v-show="loading"><div id="loading-circle"><h2>LOADING</h2><p>加载过慢请开启缓存 浏览器默认开启</p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409201808995.gif"></div></div></transition><div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}"><nav id="desktop-menu"><a class="title" href="/"><span>ORION BLOG</span></a><a href="/"><i class="fa-solid fa-house fa-fw"></i> <span>&ensp;Home</span></a><a href="/about"><i class="fa-solid fa-id-card fa-fw"></i> <span>&ensp;About</span></a><a href="/archives"><i class="fa-solid fa-box-archive fa-fw"></i> <span>&ensp;Archives</span></a><a href="/categories"><i class="fa-solid fa-bookmark fa-fw"></i> <span>&ensp;Categories</span></a><a href="/tags"><i class="fa-solid fa-tags fa-fw"></i> <span>&ensp;Tags</span></a></nav><nav id="mobile-menu"><div class="title" @click="showMenuItems = !showMenuItems"><i class="fa-solid fa-bars fa-fw"></i> <span>&emsp;ORION BLOG</span></div><transition name="slide"><div class="items" v-show="showMenuItems"><a href="/"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-house fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Home</div></div></a><a href="/about"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-id-card fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">About</div></div></a><a href="/archives"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-box-archive fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Archives</div></div></a><a href="/categories"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-bookmark fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Categories</div></div></a><a href="/tags"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-tags fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Tags</div></div></a></div></transition></nav></div><transition name="fade"><div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div></transition><div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'"><div class="article"><div><h1>Video Understanding with LLMs</h1></div><div class="info"><span class="date"><span class="icon"><i class="fa-solid fa-calendar fa-fw"></i></span> 2024/9/14</span><span class="category"><a href="/categories/%E8%AF%BB%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97/"><span class="icon"><i class="fa-solid fa-bookmark fa-fw"></i></span> 读论文系列</a></span><span class="tags"><span class="icon"><i class="fa-solid fa-tags fa-fw"></i></span> <span class="tag"><a href="/tags/%E7%BB%BC%E8%BF%B0/" style="color:#03a9f4">综述</a></span> <span class="tag"><a href="/tags/%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="color:#00a596">视频大模型</a></span></span></div><div class="content" v-pre><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058265.png" alt="image-20240914113409026"></p><p>论文发布日期:2024-07-24</p><span id="more"></span><h1><span id="abstract">Abstract</span><a href="#abstract" class="header-anchor"></a></h1><p>Vid-LLMs的三个主类（按方法分）：</p><ul><li><p>Video Analyzer × LLM</p></li><li><p>Video Embedder × LLM</p></li><li><p>(Analyzer + Embedder) × LLM</p></li></ul><p>Vid-LLMs的五个子类（按功能分）：</p><ul><li><p>LLM as Summarizer</p></li><li><p>LLM as Manager</p></li><li><p>LLM as Text Decoder</p></li><li><p>LLM as Regressor</p></li><li><p>LLM as Hidden Layer</p></li></ul><h1><span id="introduction">Introduction</span><a href="#introduction" class="header-anchor"></a></h1><p>A. 视频理解方法的发展历程</p><ol type="1"><li><p>传统方法：</p><ul><li>SIFT、SURF、HOG、背景扣除、光流、IDT（光流？）、HMM、SVM、决策树、随机森林、PCA</li></ul></li><li><p>早期神经网络视频模型：</p><ul><li><p>Deepvideo，引入深度CNN，打不过手工特征方法，因为失去了运动特征</p></li><li><p>Two-stream networks，结合了CNN和光流来捕获运动信息，能与手工打平</p></li><li><p>为了处理长视频，LSTM、TSN、FV编码、双线性编码、VLAD，显著提高了UCF101和HMDB51性能</p></li><li><p>3D卷积出现打开了新局面，I3D-&gt;R3D、MFNet、STC出现，显著提高性能，出现K400和something-something数据集评估更具挑战的性能</p></li><li><p>3D卷积分解为2D卷积和1D卷积，S3D、ECO、P3D</p></li><li><p>LTC、T3D、Non-local、V4D专注于长时间建模，CSN、SlowFast、X3D倾向于获得高效率</p></li><li><p>ViT引入又促进了一系列新的模型诞生，例如TimeSformer、VidTr、ViViT、MViT</p></li></ul></li><li><p>自监督视频预训练模型：</p><ul><li><p>VideoBERT, 基于双向语言模型BERT，相关任务用于自监督video-text数据。特征处理中，VideoBERT利用了层次化的k-means对视频特征进行分词</p></li><li><p>遵循预训练微调范式，出现了许多针对视频理解的预训练模型，分为使用不同的架构（ActBERT, SpatiotemporalMAE, OmniMAE, VideoMAE, MotionMAE）或者不同的训练策略（MaskFeat, VLM,</p><p>ALPRO, All-in-One transformer, MaskViT, CLIP-ViP,Singularity, LF-VILA, EMCL, HiTeA, CHAMPAGNE）</p></li></ul></li><li><p>大语言模型 for 视频理解</p><ul><li><p>Visual-ChatGPT使用LLM调用视觉模型API解决问题</p></li><li><p>指令微调的出现进一步增强了这些模型有效相应请求的能力</p></li></ul></li></ol><p>B.相关综述（动机）</p><p>现有综述的局限性：</p><ul><li><p>专注于特定子任务：一些论文仅研究视频理解领域的特定子任务，而不是全面概述</p></li><li><p>关注范围更广的方法：一些论文关注的方法超出了视频理解的范畴</p></li></ul><p>尽管现有的调查论文对学术界有重要价值，但它们在基于大型语言模型的一般视频理解任务调查方面留下了空白，这篇论文旨在填补这一空白，通过全面调查使用大型语言模型进行视频理解的任务来弥补现有研究的不足。</p><p>C.综述结构</p><ol type="1"><li><p>Section II:提供初步认识for video understanding with LLMs，包括不同粒度级别的各种视频理解任务、相关数据集和评估指标，还有LLMs的背景知识</p></li><li><p>Section III:深挖最近利用LLMs做视频理解的研究的细节，简述方法和影响，分为摘要说的三大类和五小类，还讲解了Vid-LLMs的训练策略</p></li><li><p>Section IV:提供更多评估Vid-LLMs的流行方法的更多信息，以及一些常用基准指标</p></li><li><p>Section V:探索Vid-LLMs的跨领域的重要应用</p></li><li><p>Section VI:总结主要发现并确定未解决的挑战和未来研究的潜在领域</p></li></ol><h1><span id="preliminaries">PRELIMINARIES</span><a href="#preliminaries" class="header-anchor"></a></h1><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058358.png" alt="image-20240914113524520"></p><p>A.视频理解任务</p><ol type="1"><li><p>Abstract Understanding Tasks</p><ul><li><p>Video Classification &amp; Action Recognition（用class labels或者activities labels分类视频）:  Top-K accuracy</p></li><li><p>Text-Video Retrieval：Recall at K (<a href="mailto:R@K" class="email">R@K</a>), which measures the accuracy of the first K retrieved results</p></li><li><p>Video-to-Text Summarization： Metrics of BLEU, METEOR, CIDEr, and ROUGE-L often evaluate this task</p></li><li><p>Video Captioning：BLEU, METEOR, CIDEr, and ROUGE-L</p></li><li><p>Video QA： Top-1, Top-K accuracy</p></li></ul></li><li><p>Temporal Understanding Tasks</p><ul><li><p>Video Summarization（将长视频压缩为短视频）：F1-score, Spearman, and Kendall usually evaluate this task as metrics</p></li><li><p>Video Highlight Detection（旨在识别和提取视频中最重要和有趣的片段）</p></li><li><p>Temporal Action/Event Localization（识别视频中动作或事件的精确时间片段）</p></li><li><p>Temporal Action Proposal Generation（生成候选片段within视频中可能包含的动作或活动）</p></li><li><p>Video Temporal Grounding（定位视频中与给定文本查询相对应的时刻或间隔）：<a href="mailto:R1@0.5" class="email">R1@0.5</a> and <a href="mailto:R1@0.7" class="email">R1@0.7</a></p></li><li><p>Moment Retrieval（识别和提取与给定文本或视觉查询对应的精确视频片段）</p></li><li><p>Generic Event Boundary Detection（识别视频中发生重大变化或者过渡的帧，根据不同的事件或活动分割视频）</p></li><li><p>Generic Event Boundary Captioning &amp; Grounding（识别和描述视频中重要事件之间的过渡点）</p></li><li><p>Dense Video Captioning（为整个视频中发生的多个事件和动作生成详细且连续的文本描述）</p></li></ul></li><li><p>Spatiotemporal Understanding Tasks</p><ul><li><p>Object Tracking（对象跟踪，持续识别特定对象的轨迹）</p></li><li><p>Re-Identification(ReID)（跨不同视频帧或摄像机识别或匹配对象）</p></li><li><p>Video Saliency Detection（识别视频中视觉上最重要且最引人注目的区域）</p></li><li><p>Video Object Segmentation（将视频划分为与各个对象相对应的片段，随时间描绘它们的边界）</p></li><li><p>Video Instance Segmentation（识别、分隔和跟踪视频中每个唯一对象）</p></li><li><p>Video Object Referring Segmentation（涉及基于自然语言描述来分割视频中的特定对象）</p></li><li><p>Spatiotemporal Grounding（根据给定的查询识别和定位视频空间和时间维度的特定对象或事件）</p></li></ul></li></ol><p>B.LLMs的背景</p><p>两个特点</p><ol type="1"><li><p>Scaling Laws</p></li><li><p>Emergent Abilities</p></li></ol><p>多模态大模型Multimodal Large Language Models (MLLMs)构成：</p><ul><li><p>multimodal encoders</p></li><li><p>cross-modality aligners</p></li><li><p>an LLM core structure</p></li></ul><h1><span id="vid-llms">VID-LLMS</span><a href="#vid-llms" class="header-anchor"></a></h1><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058567.png" alt="image-20240914113612625"></p><p>A. Taxonomy</p><ol type="1"><li><p>Video Analyzer × LLM：视频分析器被定义为一个模块，接收视频输入并输出视频分析（一般以文本形式）。文本中可能包括视频字幕、密集视频字幕（带有时间戳的视频中所有事件的详细描述）、对象跟踪结果（对象的标签、ID和边界框）以及视频中存在的其他模式的转录（ASR语音识别结果或OCR的字幕识别结果）。视频文本可以直接输入到后续的LLM中，插入输入LLM之前预先准备的模板或者转换为临时数据库以供LLM检索</p><ul><li><p>LLM as Summarizer：LLMs的主要功能是总结分析从分析器获得的结果。总结根据prompts不同而不同，有高度浓缩的摘要文本和标题或回答特定问题的综合摘要等。在这种系统中，信息通常是单向的</p></li><li><p>LLM as Manager：LLMs的主要功能是协调整个系统的运作。它可以根据用户需求主动生成命令来调用不同的视频分析器，然后在输出结果之前选择自己进一步处理该分析或者与视频分析器进行多轮交互，相比Summarizer更加灵活</p></li></ul></li><li><p>Video Embedder × LLM：Video Embedder主要指视觉编码器，比如ViT或者CLIP，用来生成视频嵌入。还有一些嵌入器对视频中其他模式进行编码，例如音频（CLAP），LLM的分词器不作为Embedder看待。与视频分析器不同，视频嵌入器的表征不能被直接使用，需要适配器将这些表征映射到LLMs的输入的文本语义空间</p><ul><li><p>LLM as Text Decoder：LLM接收视频嵌入并根据prompts或instructions将其解码为文本输出，一般这些任务不需要细粒度的理解或者精准的时空定位，仅仅主要关注一般的QA或captioning。</p></li><li><p>LLM as Regressor：与文本解码器不同，LLM作为自回归器可以预测连续值，例如视频中的时间戳定位和对象轨迹的边界框坐标（功能类似于回归任务中的回归器，尽管根本上执行的是分类？）。</p></li><li><p>LLM as Hidden Layer：LLM接收视频嵌入作为输入但不直接输出文本，而是连接到专门设定的特定任务头来执行实际的回归任务，例如视频中的事件定位或对象边界框预测，同时还会保留LLM的文本输出能力。</p></li></ul></li><li><p>(Analyzer + Embedder) × LLM：此类Vid-LLMs比较少，它可以同时获取分析器来获取视频的文本分析，并且使用视频嵌入器将视频编码为嵌入。LLM接收两种类型的输入以及其他prompt或instruction，并输出相应。子类可以为Summarizer/Manager/Text Decoder/Regressor/Hidden Layer中的任意一个</p></li></ol><p>B.Training Strategies for Vid-LLMs</p><ol type="1"><li><p>Training-free Vid-LLMs：许多Vid-LLM系统建立在极其强大的LLM之上，具有强大的零样本、情景学习和思维链的能力。大多数Video Analyzer × LLM类别的Vid-LLM无需培训，因为已经将所有视频信息解析为了文本。因为LLM可以将几乎所有NLP任务统一为生成任务，因此还可以处理许多其他视频理解任务。代表：SlowFast-LLaVA。</p></li><li><p>Fine-tuning Vid-LLMs：与Video Analyzer × LLM不同，所有Video Embedder × LLM类别中的Vid-LLM几乎都需要微调。常用微调方法有：LLM Fully Fine-tuning，Connective Adapter Fine-tuning，Insertive Adapter Fine-tuning，and Fine-tuning with Hybrid Adapters（an adapter is a small，trainable module，用于减少参数训练量）</p><ul><li><p>LLM Fully Fine-tuning：一般优于adapter微调版本，但是计算资源消耗量大。</p></li><li><p>Connective Adapter Fine-tuning：Connective Adapter一般指的是适配器，即将视频嵌入映射到文本语义空间的Adapter。在训练空间中，冻结LLM和Embedder，仅仅微调适配器。</p></li><li><p>Insertive Adapter Fine-tuning：Insertive Adapter基于LoRA，是插入在LLM本身的小Adapter单元，会影响LLM的行为，但是原本的LLM和Embedder会被冻结。一般存在于Video Embedder × LLM as Regressor and Video Embedder × LLM as Hidden Layer两类中，因为这两类需要改变LLM的行为去预测连续值。</p></li><li><p>Fine-tuning with Hybrid Adapters：使用Connective Adapter和Insertive Adapter两种组合来实现模态对齐和LLM固有行为改变。常见方法是在第一阶段仅微调Connective Adapter来实现模态对齐，第二阶段冻结Connective Adapter，改变训练任务（对齐任务到目标任务）和训练数据（对齐数据到目标数据），然后微调Insertive Adapter。还有一些同时更新两种Adapter的单阶段方法。</p></li></ul></li></ol><h1><span id="benchmarks-and-evaluation">BENCHMARKS AND EVALUATION</span><a href="#benchmarks-and-evaluation" class="header-anchor"></a></h1><p>A.Closed-set Evaluation</p><p>Closed-set evaluations 的基础是带有预定答案的问题。对于QA任务，问题被设计为多选题，然后评估正确率。对于captioning或者summarization任务，the ground truth是被提前定义的。The CIDEr, METEOR,ROUGE and SPICE metrics are computed by comparing the predicted sentences with the ground-truth sentences。</p><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058696.png" alt="image-20240914113629035"></p><p>B.Open-set Evaluation</p><p>与Closed-set Evaluation不同，Open-set Evaluation不依赖于预定义的选项。它通过将GPT-3.5/4等模型的预测与自己的答案比较，为预测分配分数。</p><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058752.png" alt="image-20240914113639017"></p><p>C.Others</p><p>需要细粒度的事件和空间的视频理解任务，比如 dense captioning, video temporal grounding, spatiotemporal grounding, object tracking, video saliency detection,等需要事件或者时空 annotations 来评估性能，一般用 IoU， <a href="mailto:Recall@K" class="email">Recall@K</a> 和 mAP。有时候用人工评估，不过太费时费力费钱。</p><h1><span id="applications">APPLICATIONS</span><a href="#applications" class="header-anchor"></a></h1><p>A.Media and Entertainment</p><ul><li><p>Online Video Platforms and Multimedia Information Retrieval</p></li><li><p>Video Summarization and Editing</p></li></ul><p>B.Interactive and User-Centric Technologies</p><ul><li><p>Virtual Education, Accessibility, and Sign Language</p></li><li><p>Interactive Gaming and Virtual Environments</p></li><li><p>State-Aware Human-Computer Interaction and Robot Planning</p></li></ul><p>C.Healthcare and Security Applications</p><ul><li><p>Healthcare Innovations（医疗保健创新）</p></li><li><p>Security, Surveillance, and Cybersecurity（安全、监控和网络安全）</p></li><li><p>Advancements in Autonomous Vehicles（自动驾驶）</p></li></ul><h1><span id="future-directions-and-conclusion">FUTURE DIRECTIONS AND CONCLUSION</span><a href="#future-directions-and-conclusion" class="header-anchor"></a></h1><p>A.Limitations and Future Work</p><ul><li><p>Fine-grained Video Understanding</p></li><li><p>Long-form Video Understanding</p></li><li><p>Multimodal Video Understanding</p></li><li><p>Human Interaction in Video Understanding</p></li><li><p>Hallucination（幻觉） in Multimodal LLMs</p></li></ul><p>B.Conclusion</p><p>主要讲述了视频理解的发展历程，不同的视频理解任务，Vid-LLMs的分类及其主要训练方法，视频任务的评估，Vid-LLMs促进的应用及其不足与未来发展。作者最后认为在以下几个方向可以继续推动进度：</p><ul><li><p>寻找更有效的训练策略</p></li><li><p>提高Vid-LLMs的规模</p></li><li><p>发展更创新的架构</p></li><li><p>扩大数据集规模和增加benchmarks</p></li></ul><p>附录</p><p>一.Vid-LLMs分类表格</p><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058823.png" alt="image-20240914113708096"></p><p>二.目前视频大模型的对比表格</p><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409180058891.png" alt="image-20240914113718395"></p></div><div id="comment"><div id="waline-container"></div></div></div><footer id="footer"><div id="footer-wrap"><div>&copy; 2024 - 2024 Orion Blog<span id="footer-icon"><i class="fa-solid fa-font-awesome fa-fw"></i></span> &commat;Orion</div><div>Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp; <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a></div></div></footer></div><transition name="fade"><div id="preview" ref="preview" v-show="previewShow"><img id="preview-content" ref="previewContent"></div></transition></div><script src="/js/main.js"></script><script>Waline.init({el:"#waline-container",serverURL:"https://www.orionverse-comment.blog/",commentCount:!0,pageview:!0,emoji:"https://unpkg.com/@waline/emojis@1.0.1/weibo,https://unpkg.com/@waline/emojis@1.0.1/alus,https://unpkg.com/@waline/emojis@1.0.1/bilibili,https://unpkg.com/@waline/emojis@1.0.1/qq,https://unpkg.com/@waline/emojis@1.0.1/tieba,https://unpkg.com/@waline/emojis@1.0.1/tw-emoji".split(","),meta:"nick,mail,link".split(","),requiredMeta:"nick".split(","),lang:"zh-CN",wordLimit:0,pageSize:"10",login:"enable"})</script></body></html>