<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><title>VideoLLaMA2 | Orion Blog</title><meta name="author" content="Orion"><meta name="description" content="Here's Orion"><meta name="keywords" content="技术博客,Orion,BUPT北京邮电大学,本科生,人工智能,AI算法工程师,机器学习,深度学习,计算机视觉,自然语言处理,NLP,CV,大模型"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><link rel="icon" href="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409190134514.png"><link rel="preload" as="image" href="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409181543316.png"><link rel="preconnect" href="https://s4.zstatic.net"><script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" media="only x" onload='this.onload=null,this.media="all"'><link rel="preconnect" href="https://fonts.googleapis.cn"><link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" media="only x" onload='this.onload=null,this.media="all"'><script>const mixins={}</script><script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script><script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"><script src="/js/lib/highlight.js"></script><script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script><script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css"><script src="/js/lib/math.js"></script><script src="/js/lib/preview.js"></script><script src="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline.min.js"></script><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline.min.css"><link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/waline/2.15.8/waline-meta.min.css"><script src="https://cdn.staticfile.org/animejs/3.2.1/anime.min.js"></script><link rel="stylesheet" href="/css/main.css"><link rel="preconnect" href="https://static-argvchs.netlify.app"><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:yellow;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style></head><body><div id="layout"><transition name="fade"><div id="loading" v-show="loading"><div id="loading-circle"><h2>LOADING</h2><p>加载过慢请开启缓存 浏览器默认开启</p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409201808995.gif"></div></div></transition><div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}"><nav id="desktop-menu"><a class="title" href="/"><span>ORION BLOG</span></a><a href="/"><i class="fa-solid fa-house fa-fw"></i> <span>&ensp;Home</span></a><a href="/about"><i class="fa-solid fa-id-card fa-fw"></i> <span>&ensp;About</span></a><a href="/archives"><i class="fa-solid fa-box-archive fa-fw"></i> <span>&ensp;Archives</span></a><a href="/categories"><i class="fa-solid fa-bookmark fa-fw"></i> <span>&ensp;Categories</span></a><a href="/tags"><i class="fa-solid fa-tags fa-fw"></i> <span>&ensp;Tags</span></a></nav><nav id="mobile-menu"><div class="title" @click="showMenuItems = !showMenuItems"><i class="fa-solid fa-bars fa-fw"></i> <span>&emsp;ORION BLOG</span></div><transition name="slide"><div class="items" v-show="showMenuItems"><a href="/"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-house fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Home</div></div></a><a href="/about"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-id-card fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">About</div></div></a><a href="/archives"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-box-archive fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Archives</div></div></a><a href="/categories"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-bookmark fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Categories</div></div></a><a href="/tags"><div class="item"><div style="min-width:20px;max-width:50px;width:10%"><i class="fa-solid fa-tags fa-fw"></i></div><div style="min-width:100px;max-width:150%;width:20%">Tags</div></div></a></div></transition></nav></div><transition name="fade"><div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div></transition><div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'"><div class="article"><div><h1>VideoLLaMA2</h1></div><div class="info"><span class="date"><span class="icon"><i class="fa-solid fa-calendar fa-fw"></i></span> 2024/9/24</span><span class="category"><a href="/categories/%E8%AF%BB%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97/"><span class="icon"><i class="fa-solid fa-bookmark fa-fw"></i></span> 读论文系列</a></span><span class="tags"><span class="icon"><i class="fa-solid fa-tags fa-fw"></i></span> <span class="tag"><a href="/tags/%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="color:#03a9f4">视频大模型</a></span> <span class="tag"><a href="/tags/VideoLLaMA2/" style="color:#ffa2c4">VideoLLaMA2</a></span></span></div><div class="content" v-pre><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241504148.png" alt="image-20240924150449075"></p><p>论文发布时间：2024.6.17</p><span id="more"></span><h1><span id="abstract">Abstract</span><a href="#abstract" class="header-anchor"></a></h1><ul><li>VideoLLaMA 2, a set of Video Large Language Models</li><li>incorporates a tailor-made <em>Spatial-Temporal Convolution</em> (STC) connector</li><li>integrate an <em>Audio Branch</em> into the model through joint training</li><li>VideoLLaMA 2 exhibits reasonable improvements in audio-only and audio-video question-answering (AQA &amp; OE-AVQA) benchmarks over existing models</li></ul><h1><span id="introduction">Introduction</span><a href="#introduction" class="header-anchor"></a></h1><p><strong>目标：</strong></p><ol type="1"><li>如何集成和解释视觉和听觉信号的复杂相互作用来增强视频语言理解。</li></ol><p><strong>动机：</strong></p><ol type="1"><li>相比于静态图片，视频融合了时间动态和同步的音频流，问题变得更加复杂。</li><li>挑战在于同时理解视觉的时间变化并且与同步音频输入相关联，目前理解音频与视觉数据的集成在当前模型汇总仍然不发达。</li><li>目前的vision-language connectors还不够合理，它们忽略了时间维度的聚合，并将时间建模的所有内容留给了语言解码器。</li></ol><p><strong>方法：</strong></p><ol type="1"><li>设计了专门的<em>Spatial-Temporal Connector</em>(STC)模块取更有效地捕获时间动态。</li><li>联合训练<em>Audio Branch</em>，增强了VideoLLaMA2的视频视觉集成能力。</li></ol><h1><span id="method">Method</span><a href="#method" class="header-anchor"></a></h1><h3><span id="architecture">Architecture</span><a href="#architecture" class="header-anchor"></a></h3><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241539448.png" alt="image-20240924153937369"></p><p>模型集成了双分支框架</p><ol type="1"><li>Vision-Language Branch</li><li>Audio-Language Branch</li></ol><p>这两个分支独立运行，以模块化方式将预训练的视觉和音频encoders与指令微调的LLMs进行连接。</p><h4><span id="vision-language-branch">Vision-Language Branch</span><a href="#vision-language-branch" class="header-anchor"></a></h4><ul><li>选用CLIP作为backbone，图像编码器与任意帧采样策略相兼容，并且支持更灵活的frame-to-video feature aggregation方案。</li><li>采用一致帧采样方法，从每个视频提取固定数量的帧。每个帧都经过填充并调整为标准336*336的尺寸，然后利用提出的STC Connector去学习时空表示。</li></ul><h4><span id="audio-language-branch">Audio-Language Branch</span><a href="#audio-language-branch" class="header-anchor"></a></h4><ul><li>音频信号通过预处理步骤转换为有128个频率段的fbank频谱图。</li><li>集成了BEATs作为音频编码器，将特征编码后通过具有两个MLP块进行处理，以与LLMs的维度保持一致。</li><li>BEAT捕获时间动态的能力与视觉分支中的STC Connector相一致，可以保证视听功能的无缝集成。</li></ul><h4><span id="large-language-model-backbone">Large Language Model Backbone</span><a href="#large-language-model-backbone" class="header-anchor"></a></h4><ul><li>与前身一样，使用instruction-following large language models (LLMs) 作为语言编码器。</li><li>没有广泛搜索最佳的LLMs，而是在所有实验中使用Mistral-Instruct和Mixtral-Instruct模型。</li></ul><h4><span id="sptial-temporal-convolution-connector">Sptial-Temporal Convolution Connector</span><a href="#sptial-temporal-convolution-connector" class="header-anchor"></a></h4><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241553077.png" alt="image-20240924155343988"></p><p>三个原则</p><ul><li>保持输出视觉token中的时空顺序</li><li>减少时空token的数量</li><li>减轻时空下采样过程中的信息丢失</li></ul><p>策略</p><ul><li>避免使用重采样结构，因为其无法保证时空顺序的保留。自回归模型（LLM backbone）高度依赖训练和推理之间的一致标记顺序，因此在构建connector时只考虑卷积和池化操作</li><li>插入3D下采样算子压缩时空token</li><li>为了补充时空下采样损失，在时空下采样之前和之后插入RegStage（强卷积块）</li></ul><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241602319.png" alt="image-20240924160204271"></p><p>实验</p><ul><li>3D卷积和STC连接器结合RegStage块效果最好。</li><li>所有3D下采样设计在Egoschema上的表现都比2D更好，表明帧级特征的早期融合有利于长视频理解。</li></ul><p>STC代码</p><pre class="python"><code>import torch.nn as nn
from timm.models.regnet import RegStage

class STCConnector(nn.Module):
def __init__(self, config, depth, mlp_depth):
# 时空下采样因子,td表示时间维度,sd表示空间维度
td, sd = config.td, config.sd
# 输入和输出隐藏层大小
in_size, out_size = config.in_size, config.out_size
# 第一个RegStage块
self.sl = RegStage(depth=depth, in_chs=in_size, out_chs=out_size)
# 3D卷积下采样
self.downsampler = nn.Conv3d(in_channels=out_size,
out_channels=out_size,
kernel_size=(td, sd, sd))
# 第二个RegStage块
self.s2 = RegStage(depth=depth, in_chs=out_size,out_chs=out_size)
self.proj = build_mlp(mlp_depth, out_size, out_size)

def forward(self, x):
x = self.sl(x)
x = self.downsampler(x)
x = self.s2(x)
x = self.proj(x)
return x</code></pre><h1><span id="training">Training</span><a href="#training" class="header-anchor"></a></h1><h3><span id="video-language-training">Video-Language Training</span><a href="#video-language-training" class="header-anchor"></a></h3><h4><span id="pre-training">Pre-training</span><a href="#pre-training" class="header-anchor"></a></h4><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241639198.png" alt="image-20240924163938147"></p><ul><li><p>利用大规模，弱标记的网络爬虫图文和视频文本对数据集</p></li><li><p>视觉编码器和LLM部分被冻结，仅优化<em>STC Connector</em></p></li><li><p>视频帧被均匀采样并调整为336*336像素</p></li><li><p>训练目标是最小化文本tokens的交叉熵损失</p></li></ul><h4><span id="multi-task-fine-tuning">Multi-task Fine-tuning</span><a href="#multi-task-fine-tuning" class="header-anchor"></a></h4><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241640031.png" alt="image-20240924164020975"></p><ul><li>同时利用视频文本和图像文本对数据合并高质量细粒度的多模态注释</li><li>针对四项任务微调</li><li>video captioning：VideoChat和内部收集数据</li><li>video classification and VQA：Kinetics-710、 SthSthv2等混合</li><li>enhance instruction-following capabilities：多个VLM的数据，例如Vally，再混合来自ShareGPT4V的图像字幕数据和来自LLaVA的图像VQA和指令跟踪数据</li><li>视觉编码器被冻结，仅优化LLMs和STC Connecter</li><li>训练目标与预训练阶段一致</li></ul><h3><span id="audio-language-training">Audio-Language Training</span><a href="#audio-language-training" class="header-anchor"></a></h3><h4><span id="pre-training">Pre-training</span><a href="#pre-training" class="header-anchor"></a></h4><ul><li>利用WavCaps数据集来关注模型的基本音频理解方面。每个样本都进行了精心注释，旨在训练模型以根据音频输入生成描述性文本。</li><li>音频编码器和大语言模型（LLM）被冻结，仅优化音频projector。</li><li>训练目标是最小化the next token prediction loss在文本响应中，增强模型理解音频数据并映射到文本表示的能力。</li></ul><h4><span id="multi-task-fine-tuning">Multi-task Fine-tuning</span><a href="#multi-task-fine-tuning" class="header-anchor"></a></h4><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241724667.png" alt="image-20240924172450615"></p><ul><li><p>多任务微调</p></li><li><p>question answering based on audio cues：ClothoAQA</p></li><li><p>instruction tuning phase： WavCaps，AudioCaps，Clotho</p></li><li><p>music-based captioning：MusicCaps</p></li><li><p>sound event classification：VGGSound等</p></li><li><p>LLM保持冻结，优化音频编码器和音频projector</p></li><li><p>保持一致的训练目标，最大限度减少预训练阶段的文本标签交叉熵损失</p></li></ul><h3><span id="audio-video-joint-trainng">Audio-Video Joint Trainng</span><a href="#audio-video-joint-trainng" class="header-anchor"></a></h3><ul><li>AVQA，AVQA-music等多个数据集</li><li>训练步骤</li></ul><ol type="1"><li>从视频中提取音轨并进行剪切以对齐视频片段</li><li>这些音频剪辑被截断或填充到与audio-language traning stage相同的持续时间</li><li>对于缺少音轨的视频，利用零填充波形保证数据样本一致性</li><li>冻结音频和视频编码器的时候，优化音频和视频projecter，不冻结LLM</li><li>训练目标与指令微调阶段保持一致</li></ol><h1><span id="implementation">Implementation</span><a href="#implementation" class="header-anchor"></a></h1><ul><li>built upon LLaVA 1.5 library （<a target="_blank" rel="noopener" href="https://github.com/haotian-liu/LLaVA" class="uri">https://github.com/haotian-liu/LLaVA</a>）</li><li>视频编码器：clip-large-336 （<a target="_blank" rel="noopener" href="https://huggingface.co/openai/clip-vit-large-patch14-336" class="uri">https://huggingface.co/openai/clip-vit-large-patch14-336</a>）</li><li>音频编码器：Fine-tuned BEATs_iter3+(AS2M)(cpt2)（<a target="_blank" rel="noopener" href="https://1drv.ms/u/s!AqeByhGUtINrgcpj8ujXH1YUtxooEg?e=E9Ncea" class="uri">https://1drv.ms/u/s!AqeByhGUtINrgcpj8ujXH1YUtxooEg?e=E9Ncea</a>）</li><li>文本编码器<ul><li>VideoLLaMA 2 (7B)：Mistral-7B-Instruct</li><li>VideoLLaMA 2 (8x7B) ：Mixtral-8x7B-Instruct</li></ul></li><li>预训练阶段和微调期间不进行超参数调整</li><li>预训练<ul><li>global batch size：1024</li><li>the learning rate：1e-3</li></ul></li><li>微调<ul><li>global batch size：2048</li><li>the learning rate：2e-5</li></ul></li><li>预训练1epoch，微调3epoch</li></ul><h1><span id="model-evaluation">Model Evaluation</span><a href="#model-evaluation" class="header-anchor"></a></h1><h3><span id="video-understanding">Video Understanding</span><a href="#video-understanding" class="header-anchor"></a></h3><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241749982.png" alt="image-20240924174930908"></p><ul><li>Evaluation Benchmarks</li><li>MC-VQA</li><li>OE-VQA</li><li>VC</li><li>Baselines</li><li>Proprietary Models：Gemini 1.0 Series等</li><li>Open-Source Models：VistaLLaMA等</li></ul><h3><span id="audio-understanding">Audio Understanding</span><a href="#audio-understanding" class="header-anchor"></a></h3><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241752744.png" alt="image-20240924175234692"></p><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241752008.png" alt="image-20240924175258950"></p><ul><li><p>Evaluation Benchmarks</p></li><li><p>AQA</p></li><li><p>OE-AVQA</p></li><li><p>Baselines</p></li><li><p>Audio Understanding：Qwen-Audio</p></li><li><p>Audio-Visual Understanding： PandaGPT等</p></li></ul><h1><span id="cases">Cases</span><a href="#cases" class="header-anchor"></a></h1><p><img src="https://testingcf.jsdelivr.net/gh/nanzhi84/blog-images/202409241755940.png" alt="image-20240924175546784"></p><h1><span id="related-work">Related Work</span><a href="#related-work" class="header-anchor"></a></h1><ul><li><p>现存的VLM通常由两方面组成</p><ul><li><p>预训练的视觉编码器或者DINO，用于将视频帧编码为低维视觉特征，搭配vision-language adapter 用于聚合这些视频特征并转换到LLMs可以理解的表示</p></li><li><p>指令微调的语言编码器或者Mistral-Instruct，用于基于指令和用户上传的视频生成文本回答</p></li></ul></li><li><p>这篇paper主要探究的vision-language connectors的更好设计</p></li><li><p>目前的vision-language adapters for Image-LLMs有Cross-Attention、Q-Former、Linear Pro-<br>jection和Dynamic Visual Tokenizer等。它们结果不错，但这些设计对于VLM仍然不够，因为它们完全忽略了时间聚合，并将时间建模的内容留给了语言解码器</p></li><li><p>本文利用STC Connector引入LLMs，同时保持合理的token数量。先利用3D卷积进行时空聚合，并且为了保留局部视觉模式，遵循Cha等人的方法，在3D卷积之前和之后合并RegStage块</p></li><li><p>在扩展VLM的核心功能上，最新的进展集中在集成音频流。</p><ul><li>PandaGPT和Video-LLaMA等模型利用ImageBind等预训练系统进行模态对齐</li><li>MacawLLM使用不同的教学集优化视觉和音频数据的单独分支。</li><li>X-InstructBLIP使用冻结的LLM在单个框架内集成多种模态，并使用特定模态的Q-Formers作为适配器来桥接各种编码器。</li><li>OneLLM引入了通用编码器和投影模块，旨在将各种模态与语言数据对齐。</li><li>CREMA采用高效和模块化方法，为每种模态利用参数高效的适配器，以增强将新模态合并入现有框架的灵活性和易用性。</li><li>AV-LLM和AVicuna等其他模型利用集成的视听训练来进一步完善对复杂多模态内容的理解。</li></ul></li></ul><h1><span id="conclusion">Conclusion</span><a href="#conclusion" class="header-anchor"></a></h1><ul><li>推出了VideoLLaMA2系列，是一组通用视频大语言模型。</li><li>精心设计了STC Connecter和联合训练的音频分支，提高了模型面向视频和音频的多模态推理。</li><li>在多个基准测试中达到了与专有模型相当的水平。</li></ul></div><div id="comment"><div id="waline-container"></div></div></div><footer id="footer"><div id="footer-wrap"><div>&copy; 2024 - 2024 Orion Blog<span id="footer-icon"><i class="fa-solid fa-font-awesome fa-fw"></i></span> &commat;Orion</div><div>Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp; <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a></div></div></footer></div><transition name="fade"><div id="preview" ref="preview" v-show="previewShow"><img id="preview-content" ref="previewContent"></div></transition></div><script src="/js/main.js"></script><script>Waline.init({el:"#waline-container",serverURL:"https://www.orionverse-comment.blog/",commentCount:!0,pageview:!0,emoji:"https://unpkg.com/@waline/emojis@1.0.1/weibo,https://unpkg.com/@waline/emojis@1.0.1/alus,https://unpkg.com/@waline/emojis@1.0.1/bilibili,https://unpkg.com/@waline/emojis@1.0.1/qq,https://unpkg.com/@waline/emojis@1.0.1/tieba,https://unpkg.com/@waline/emojis@1.0.1/tw-emoji".split(","),meta:"nick,mail,link".split(","),requiredMeta:"nick".split(","),lang:"zh-CN",wordLimit:0,pageSize:"10",login:"enable"})</script></body></html>